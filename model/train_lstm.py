# model/train_lstm.py
import torch
import torch.nn as nn
import numpy as np
from pymongo import MongoClient
from sklearn.preprocessing import MinMaxScaler
import joblib

# âš™ï¸ Cáº¥u hÃ¬nh
SEQ_LEN = 60  # 60 phÃºt gáº§n nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n
BATCH_SIZE = 16
EPOCHS = 32
LR = 0.001


# ğŸ§  MÃ´ hÃ¬nh LSTM Ä‘Æ¡n giáº£n
class LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=4):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)  # â¬…ï¸ Dá»± Ä‘oÃ¡n nhiá»u bÆ°á»›c

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # láº¥y output táº¡i bÆ°á»›c cuá»‘i cÃ¹ng
        return self.fc(out)



# ğŸ“¥ Táº£i dá»¯ liá»‡u tá»« MongoDB
def load_data():
    client = MongoClient('mongodb://localhost:27017/')
    collection = client['realtime']['btc_prices']
    docs = list(collection.find().sort('timestamp', 1))
    prices = np.array([d['price'] for d in docs], dtype=np.float32)
    return prices


# âœ‚ï¸ Táº¡o táº­p huáº¥n luyá»‡n tá»« chuá»—i thá»i gian
def create_sequences(data, seq_len=60):
    X = []
    y = []
    for i in range(len(data) - seq_len - 1440):  # cáº§n Ä‘á»§ xa cho t+1440
        seq_x = data[i:i+seq_len]
        # Táº¡o nhÃ£n táº¡i cÃ¡c bÆ°á»›c: +15p, +1h, +3h, +1d
        seq_y = [
            data[i + seq_len + 15],
            data[i + seq_len + 60],
            data[i + seq_len + 180],
            data[i + seq_len + 1440]
        ]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)


# ğŸš‚ Huáº¥n luyá»‡n mÃ´ hÃ¬nh
def train():
    prices = load_data()

    if len(prices) <= SEQ_LEN + 1440:
        raise ValueError(f"Chá»‰ cÃ³ {len(prices)} Ä‘iá»ƒm dá»¯ liá»‡u. Cáº§n Ã­t nháº¥t {SEQ_LEN + 1440} Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»›i 1 ngÃ y.")

    # ğŸ“‰ Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = MinMaxScaler()
    norm_prices = scaler.fit_transform(prices.reshape(-1, 1)).flatten()

    # ğŸ¯ Táº¡o táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vá»›i nhiá»u nhÃ£n Ä‘áº§u ra
    X, y = create_sequences(norm_prices)  # y shape: (samples, 4)
    X = torch.tensor(X[:, :, None], dtype=torch.float32)   # (samples, seq_len, 1)
    y = torch.tensor(y, dtype=torch.float32)               # (samples, 4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LSTM(output_size=4).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    X = X.to(device)
    y = y.to(device)

    previous_losses = []
    stable_epochs = 0
    tolerance = 1e-6

    for epoch in range(EPOCHS):
        model.train()
        permutation = torch.randperm(X.size(0))
        losses = []

        for i in range(0, X.size(0), BATCH_SIZE):
            indices = permutation[i:i + BATCH_SIZE]
            batch_x, batch_y = X[indices], y[indices]

            optimizer.zero_grad()
            output = model(batch_x)  # (batch_size, 4)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        avg_losses = np.mean(losses)
        print(f"Epoch {epoch + 1}/{EPOCHS}, Loss: {avg_losses:.6f}")

        if previous_losses and abs(avg_losses - previous_losses[-1]) < tolerance:
            stable_epochs += 1
        else:
            stable_epochs = 0
        previous_losses.append(avg_losses)

        if stable_epochs > 3:
            break

    # ğŸ’¾ LÆ°u mÃ´ hÃ¬nh vÃ  scaler
    torch.save(model.state_dict(), 'model/lstm_multi.pt')
    joblib.dump(scaler, 'model/scaler.pkl')
    print("âœ… MÃ´ hÃ¬nh nhiá»u bÆ°á»›c vÃ  scaler Ä‘Ã£ Ä‘Æ°á»£c lÆ°u.")



if __name__ == '__main__':
    train()
